# pytorch_lightning_inference.py
"""
PyTorch Lightning LSTM –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —á–µ—Ä–µ–∑ MLflow Serving (–±–µ–∑ ONNX).
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç PyTorch Lightning –º–æ–¥–µ–ª—å –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ MLflow registry.
"""

import json
import subprocess
import time
import traceback
from collections import defaultdict
from pathlib import Path
from typing import Any, Dict, List, Tuple
from warnings import warn

import hydra
import mlflow
import mlflow.pytorch
import numpy as np
import requests
import torch
import torch.nn as nn
from hydra import utils
from joblib import load as joblib_load
from omegaconf import DictConfig
from sklearn.metrics import classification_report
from torch.utils.data import DataLoader, Dataset

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ –≤–∞—à–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞
from ...paths.paths import NAMES
from .load import (
    CLASS_NAME_TO_LABEL_MAP,
    CLASS_NAMES_ORDERED,
    LABEL_TO_CLASS_NAME_MAP,
    NUM_CLASSES,
)


class InferenceGaitSequenceDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ LSTM –º–æ–¥–µ–ª–∏."""

    def __init__(
        self, sequences: List[torch.Tensor], labels: List[int], file_ids: List[str]
    ):
        if not (len(sequences) == len(labels) == len(file_ids)):
            raise ValueError("–î–ª–∏–Ω—ã sequences, labels –∏ file_ids –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å")

        self.sequences = sequences
        self.labels = labels
        self.file_ids = file_ids

        seq_shape = sequences[0].shape if sequences else "(–ø—É—Å—Ç–æ)"
        print(
            f"–°–æ–∑–¥–∞–Ω Inference –¥–∞—Ç–∞—Å–µ—Ç —Å {len(self.sequences)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏. –§–æ—Ä–º–∞: {seq_shape}"
        )

    def __len__(self) -> int:
        return len(self.sequences)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int, str]:
        return self.sequences[idx], self.labels[idx], self.file_ids[idx]


def load_sequences_for_inference(
    feature_dir: Path,
    names_structure: List[Dict[str, Any]],
    class_map: Dict[str, int],
    seq_length: int,
    stride: int,
    input_size_per_frame: int,
) -> Tuple[List[torch.Tensor], List[int], List[str]]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç .npy —Ñ–∞–π–ª—ã –∏ —Å–æ–∑–¥–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞."""

    all_sequences, all_true_labels, all_file_identifiers = [], [], []
    print(f"\n–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏–∑ {feature_dir}...")

    processed_files, skipped_files = 0, 0

    # –°–æ–∑–¥–∞–µ–º –∫–∞—Ä—Ç—É filename -> label
    filename_to_label_map: Dict[str, int] = {}
    for class_info in names_structure:
        class_name = class_info.get("class")
        label = class_map.get(class_name)
        if label is None:
            continue
        for sample in class_info.get("samples", []):
            output_name = sample.get("out")
            if output_name:
                filename_to_label_map[output_name] = label

    print(f"–°–æ–∑–¥–∞–Ω–∞ –∫–∞—Ä—Ç–∞ –∏–º—è_—Ñ–∞–π–ª–∞ -> –º–µ—Ç–∫–∞ –¥–ª—è {len(filename_to_label_map)} —Ñ–∞–π–ª–æ–≤")

    if not feature_dir.is_dir():
        warn(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {feature_dir}")
        return [], [], []

    # –ó–∞–≥—Ä—É–∂–∞–µ–º .npy —Ñ–∞–π–ª—ã
    for filename in sorted(feature_dir.glob("*.npy")):
        base_name = filename.stem
        true_label = filename_to_label_map.get(base_name)

        if true_label is None:
            warn(f"–ù–µ –Ω–∞–π–¥–µ–Ω–∞ –º–µ—Ç–∫–∞ –¥–ª—è —Ñ–∞–π–ª–∞ {base_name}")
            skipped_files += 1
            continue

        try:
            data = np.load(filename).astype(np.float32)

            if (
                data.ndim != 2
                or data.shape[0] < seq_length
                or data.shape[1] != input_size_per_frame
            ):
                raise ValueError(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Ñ–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {data.shape}")

            # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ —Ñ–∞–π–ª–∞
            sequences_from_file = []
            for i in range(0, data.shape[0] - seq_length + 1, stride):
                sequence = data[i : i + seq_length, :]
                sequences_from_file.append(torch.tensor(sequence, dtype=torch.float32))

            if sequences_from_file:
                num_seqs = len(sequences_from_file)
                all_sequences.extend(sequences_from_file)
                all_true_labels.extend([true_label] * num_seqs)
                all_file_identifiers.extend([base_name] * num_seqs)
                processed_files += 1
            else:
                warn(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ {filename}")
                skipped_files += 1

        except Exception as e:
            warn(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {filename}: {e}")
            skipped_files += 1

    print(f"–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {processed_files}")
    print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {skipped_files}")
    print(f"–í—Å–µ–≥–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {len(all_sequences)}")

    return all_sequences, all_true_labels, all_file_identifiers


def start_mlflow_serving(cfg: DictConfig, model_uri: str) -> subprocess.Popen:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç MLflow serving —Å–µ—Ä–≤–µ—Ä –¥–ª—è PyTorch Lightning –º–æ–¥–µ–ª–∏."""

    print(f"üöÄ –ó–∞–ø—É—Å–∫ MLflow serving —Å–µ—Ä–≤–µ—Ä–∞...")
    print(f"üì¶ Model URI: {model_uri}")
    print(f"üåê Endpoint: http://{cfg.mlflow.serving.host}:{cfg.mlflow.serving.port}")

    cmd = [
        "mlflow",
        "models",
        "serve",
        "-m",
        model_uri,
        "-h",
        cfg.mlflow.serving.host,
        "-p",
        str(cfg.mlflow.serving.port),
        "--workers",
        str(cfg.mlflow.serving.workers),
        "--no-conda",
    ]

    try:
        # –ó–∞–ø—É—Å–∫ –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ
        process = subprocess.Popen(
            cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
        )

        # –ñ–¥–µ–º –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞
        print("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞...")
        time.sleep(15)  # ‚úÖ –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –¥–ª—è PyTorch –º–æ–¥–µ–ª–µ–π

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å
        health_url = f"http://{cfg.mlflow.serving.host}:{cfg.mlflow.serving.port}/ping"
        for attempt in range(10):  # ‚úÖ –ë–æ–ª—å—à–µ –ø–æ–ø—ã—Ç–æ–∫
            try:
                response = requests.get(health_url, timeout=5)
                if response.status_code == 200:
                    print("‚úÖ MLflow serving —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                    return process
            except requests.exceptions.RequestException:
                print(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/10...")
                time.sleep(3)

        raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–∂–¥–∞—Ç—å—Å—è –∑–∞–ø—É—Å–∫–∞ MLflow —Å–µ—Ä–≤–µ—Ä–∞")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ MLflow serving: {e}")
        raise


def predict_via_mlflow_serving(
    sequences_batch: np.ndarray, endpoint_url: str, timeout: int = 30
) -> np.ndarray:
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —á–µ—Ä–µ–∑ MLflow serving API –¥–ª—è PyTorch Lightning –º–æ–¥–µ–ª–∏."""

    # ‚úÖ –§–û–†–ú–ê–¢ –î–õ–Ø PYTORCH LIGHTNING –ú–û–î–ï–õ–ò
    sequences_batch_f32 = sequences_batch.astype(np.float32)

    data_dict = {
        "instances": sequences_batch_f32.tolist()  # PyTorch –º–æ–¥–µ–ª–∏ –æ–∂–∏–¥–∞—é—Ç instances
    }

    headers = {"Content-Type": "application/json"}

    try:
        response = requests.post(
            endpoint_url, data=json.dumps(data_dict), headers=headers, timeout=timeout
        )

        if response.status_code != 200:
            print(f"‚ùå Status Code: {response.status_code}")
            print(f"‚ùå Response Text: {response.text}")
            print(f"‚ùå Request Data Shape: {sequences_batch.shape}")
            print(f"‚ùå Request Data Type: {sequences_batch_f32.dtype}")

        response.raise_for_status()

        predictions = response.json()

        # ‚úÖ –û–ë–†–ê–ë–û–¢–ö–ê –û–¢–í–ï–¢–ê –û–¢ PYTORCH LIGHTNING –ú–û–î–ï–õ–ò
        if "predictions" in predictions:
            result = np.array(predictions["predictions"])
        elif isinstance(predictions, list):
            result = np.array(predictions)
        else:
            result = np.array(predictions)

        print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è. –§–æ—Ä–º–∞: {result.shape}")
        return result

    except requests.exceptions.RequestException as e:
        print(f"‚ùå HTTP –æ—à–∏–±–∫–∞: {e}")
        if hasattr(e, "response") and e.response is not None:
            print(f"‚ùå Response content: {e.response.text}")
        raise RuntimeError(f"–û—à–∏–±–∫–∞ HTTP –∑–∞–ø—Ä–æ—Å–∞: {e}")
    except (KeyError, ValueError, json.JSONDecodeError) as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
        raise RuntimeError(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞: {e}")


def run_mlflow_inference_with_margin(
    dataloader: DataLoader, endpoint_url: str, batch_size: int, timeout: int = 30
) -> List[Dict[str, Any]]:
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —á–µ—Ä–µ–∑ MLflow serving —Å —Ä–∞—Å—á–µ—Ç–æ–º margin."""

    print("\n--- –ó–∞–ø—É—Å–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ —á–µ—Ä–µ–∑ MLflow serving ---")

    # ‚úÖ –¢–ï–°–¢–û–í–´–ô –ó–ê–ü–†–û–°
    print("üß™ –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å –æ–¥–Ω–∏–º –ø—Ä–∏–º–µ—Ä–æ–º...")
    test_batch = next(iter(dataloader))
    test_sequences, _, _ = test_batch
    test_sample = test_sequences[:1].numpy()

    try:
        test_output = predict_via_mlflow_serving(test_sample, endpoint_url, timeout)
        print(f"‚úÖ –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —É—Å–ø–µ—à–µ–Ω. –í—ã—Ö–æ–¥: {test_output.shape}")
        print(f"üìä –ü—Ä–∏–º–µ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {test_output[0][:5]}")
        print(f"üìä –ü—Ä–∏–º–µ—Ä –ª–æ–≥–∏—Ç–æ–≤ (—Å—É–º–º–∞): {test_output[0].sum():.4f}")
    except Exception as e:
        print(f"‚ùå –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –Ω–µ—É–¥–∞—á–µ–Ω: {e}")
        return []

    results: List[Dict[str, Any]] = []
    processed_sequences_count = 0

    for batch_idx, (sequences, true_labels_batch, file_ids_batch) in enumerate(
        dataloader
    ):
        batch_actual_size = sequences.size(0)
        sequences_np = sequences.numpy()

        try:
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —á–µ—Ä–µ–∑ MLflow API
            outputs = predict_via_mlflow_serving(sequences_np, endpoint_url, timeout)

            # ‚úÖ –†–ê–°–ß–ï–¢ –í–ï–†–û–Ø–¢–ù–û–°–¢–ï–ô –ò MARGIN (–∫–∞–∫ –≤ paste.txt)
            if outputs.ndim == 2:
                probs_batch = torch.softmax(torch.tensor(outputs), dim=1).numpy()
            else:
                probs_batch = outputs  # –ï—Å–ª–∏ —É–∂–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏

            # –†–∞—Å—á–µ—Ç margin (—Ä–∞–∑–Ω–æ—Å—Ç—å –º–µ–∂–¥—É —Ç–æ–ø-1 –∏ —Ç–æ–ø-2)
            top_k_indices = np.argsort(probs_batch, axis=1)[:, -2:]  # –¢–æ–ø-2 –∏–Ω–¥–µ–∫—Å–∞
            top_probs = probs_batch[
                np.arange(batch_actual_size), top_k_indices[:, -1]
            ]  # –¢–æ–ø-1
            second_probs = probs_batch[
                np.arange(batch_actual_size), top_k_indices[:, -2]
            ]  # –¢–æ–ø-2
            margins = top_probs - second_probs
            preds = top_k_indices[:, -1]  # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è = —Ç–æ–ø-1 –∏–Ω–¥–µ–∫—Å—ã

            # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            true_labels_np = np.array(true_labels_batch)

            for i in range(batch_actual_size):
                results.append(
                    {
                        "predicted_label": preds[i],
                        "true_label": true_labels_np[i],
                        "file_id": file_ids_batch[i],
                        "margin": margins[i],
                        "top1_prob": top_probs[i],
                        "probabilities": probs_batch[i],
                    }
                )

            processed_sequences_count += batch_actual_size

            if (batch_idx + 1) % 10 == 0:
                print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –±–∞—Ç—á–µ–π: {batch_idx + 1}/{len(dataloader)}")

        except Exception as e:
            warn(f"–û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ {batch_idx}: {e}")
            continue

    print(
        f"–ò–Ω—Ñ–µ—Ä–µ–Ω—Å –∑–∞–≤–µ—Ä—à–µ–Ω. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {processed_sequences_count}"
    )
    return results


def aggregate_and_report_margin_voting(
    all_sequence_results: List[Dict[str, Any]],
    label_to_name_map: Dict[int, str],
    num_classes: int,
    margin_threshold: float = 0.0,
    top_k: int = 3,
) -> Tuple[List[int], List[int], Dict[str, Dict]]:
    """–ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å margin sum voting –∫–∞–∫ –≤ paste.txt."""

    if not all_sequence_results:
        warn("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏")
        return [], [], {}

    print(
        f"\n--- –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å Margin Sum Voting (–ø–æ—Ä–æ–≥={margin_threshold}) ---"
    )

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ margin
    filtered_results = [
        res for res in all_sequence_results if res["margin"] >= margin_threshold
    ]

    total_sequences = len(all_sequence_results)
    filtered_count = len(filtered_results)

    if total_sequences > 0:
        print(
            f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {filtered_count}/{total_sequences} "
            f"({filtered_count/total_sequences*100:.1f}%)"
        )

    if filtered_count == 0:
        warn("–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
        return [], [], {}

    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ file_id
    file_data = defaultdict(
        lambda: {"preds_margins": [], "probs": [], "true_label": -1}
    )

    for res in filtered_results:
        file_id = res["file_id"]
        if file_data[file_id]["true_label"] == -1:
            file_data[file_id]["true_label"] = res["true_label"]

        file_data[file_id]["preds_margins"].append(
            (res["predicted_label"], res["margin"])
        )
        file_data[file_id]["probs"].append(res["probabilities"])

    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Å margin sum voting
    file_summaries: Dict[str, Dict] = {}
    final_true_labels: List[int] = []
    final_predicted_labels: List[int] = []
    correct_files_count = 0

    print("\n" + "=" * 50 + " –û—Ç—á–µ—Ç –ø–æ —Ñ–∞–π–ª–∞–º " + "=" * 50)

    for file_id, data in sorted(file_data.items()):
        true_label = data["true_label"]
        true_name = label_to_name_map.get(true_label, f"Label_{true_label}")
        filtered_sequences_count = len(data["preds_margins"])

        # Margin sum voting
        margin_sums = np.zeros(num_classes, dtype=np.float64)
        for pred_label, margin_val in data["preds_margins"]:
            if 0 <= pred_label < num_classes:
                margin_sums[pred_label] += margin_val

        final_pred_label = np.argmax(margin_sums)
        final_pred_name = label_to_name_map.get(
            final_pred_label, f"Label_{final_pred_label}"
        )
        final_pred_score = margin_sums[final_pred_label]

        # –¢–æ–ø-K –ø–æ —Å—Ä–µ–¥–Ω–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        avg_prob_dist = np.mean(data["probs"], axis=0)
        top_indices = np.argsort(avg_prob_dist)[-top_k:][::-1]

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏
        is_correct = final_pred_label == true_label
        if is_correct:
            correct_files_count += 1

        status_str = "[–ü—Ä–∞–≤–∏–ª—å–Ω–æ]" if is_correct else "[–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ]"

        # –í—ã–≤–æ–¥ –æ—Ç—á–µ—Ç–∞ –¥–ª—è —Ñ–∞–π–ª–∞
        print(f"\n–§–∞–π–ª: {file_id}")
        print(f"  –ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å: {true_name}")
        print(f"  –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ (Margin Sum): {final_pred_name} {status_str}")
        print(f"  –ò—Ç–æ–≥–æ–≤—ã–π —Å—á–µ—Ç: {final_pred_score:.4f}")
        print(f"  –¢–æ–ø-{top_k} –∫–ª–∞—Å—Å–æ–≤ (–ø–æ —Å—Ä–µ–¥–Ω–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏):")
        for i, k_idx in enumerate(top_indices):
            k_name = label_to_name_map.get(k_idx, f"Label_{k_idx}")
            k_prob = avg_prob_dist[k_idx]
            print(f"    {i+1}. {k_name}: {k_prob:.4f}")
        print(f"  –£—á—Ç–µ–Ω–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {filtered_sequences_count}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
        final_true_labels.append(true_label)
        final_predicted_labels.append(final_pred_label)
        file_summaries[file_id] = {
            "true_label": true_label,
            "true_name": true_name,
            "predicted_label": final_pred_label,
            "predicted_name": final_pred_name,
            "predicted_score": float(final_pred_score),
            "is_correct": is_correct,
            "num_sequences_considered": filtered_sequences_count,
        }

    print("=" * 120)

    # –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
    total_files = len(file_summaries)
    if total_files > 0:
        file_accuracy = correct_files_count / total_files
        print(
            f"\n–ò—Ç–æ–≥–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ñ–∞–π–ª–æ–≤: {file_accuracy:.4f} "
            f"({correct_files_count}/{total_files})"
        )

    return final_true_labels, final_predicted_labels, file_summaries


@hydra.main(
    config_path="../../../../configs/inference",
    config_name="mlflow_lstm",
    version_base="1.1",
)
def main(cfg: DictConfig):
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ PyTorch Lightning –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ MLflow."""

    print("=== PyTorch Lightning LSTM –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —á–µ—Ä–µ–∑ MLflow Serving ===")
    print(f"Endpoint: http://{cfg.mlflow.serving.host}:{cfg.mlflow.serving.port}")
    print(f"–ú–æ–¥–µ–ª—å: {cfg.mlflow.model.model_uri}")
    print("=" * 50)

    # –ü–æ–ª—É—á–∞–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞
    project_root = Path(utils.get_original_cwd())

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ MLflow
    mlflow.set_tracking_uri("http://127.0.0.1:8080")  # Tracking server

    serving_process = None

    try:
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ scaler
        print("\n--- –ó–∞–≥—Ä—É–∑–∫–∞ Scaler ---")
        scaler_path = project_root / cfg.data.scaler_path

        if not scaler_path.exists():
            raise FileNotFoundError(f"Scaler –Ω–µ –Ω–∞–π–¥–µ–Ω: {scaler_path}")

        scaler = joblib_load(scaler_path)
        print(f"‚úÖ Scaler –∑–∞–≥—Ä—É–∂–µ–Ω: {scaler_path}")

        # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        print("\n--- –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ ---")
        feature_dir = project_root / cfg.data.input_dir

        sequences, true_labels, file_identifiers = load_sequences_for_inference(
            feature_dir=feature_dir,
            names_structure=NAMES,
            class_map=CLASS_NAME_TO_LABEL_MAP,
            seq_length=cfg.data.sequence_length,
            stride=cfg.data.stride,
            input_size_per_frame=cfg.data.input_size_per_frame,
        )

        if not sequences:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞")
            return

        # 3. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        print("\n--- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö ---")
        scaled_sequences = []
        for seq in sequences:
            seq_np = seq.numpy().astype(np.float32)
            seq_scaled = scaler.transform(seq_np).astype(np.float32)
            scaled_sequences.append(torch.tensor(seq_scaled, dtype=torch.float32))

        print(f"‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ {len(scaled_sequences)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")

        # 4. –°–æ–∑–¥–∞–Ω–∏–µ DataLoader
        print("\n--- –°–æ–∑–¥–∞–Ω–∏–µ DataLoader ---")
        inference_dataset = InferenceGaitSequenceDataset(
            scaled_sequences, true_labels, file_identifiers
        )

        inference_loader = DataLoader(
            inference_dataset,
            batch_size=cfg.inference.batch_size,
            shuffle=False,
            num_workers=cfg.inference.num_workers,
            pin_memory=cfg.inference.pin_memory,
        )

        # 5. –ó–∞–ø—É—Å–∫ MLflow serving
        serving_process = start_mlflow_serving(cfg, cfg.mlflow.model.model_uri)

        # 6. –ò–Ω—Ñ–µ—Ä–µ–Ω—Å —á–µ—Ä–µ–∑ MLflow API
        endpoint_url = cfg.client.endpoint_url

        all_sequence_results = run_mlflow_inference_with_margin(
            dataloader=inference_loader,
            endpoint_url=endpoint_url,
            batch_size=cfg.client.max_batch_size,
            timeout=cfg.client.timeout_seconds,
        )

        # 7. –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if all_sequence_results:
            (
                final_true_labels,
                final_predicted_labels,
                file_summaries,
            ) = aggregate_and_report_margin_voting(
                all_sequence_results=all_sequence_results,
                label_to_name_map=LABEL_TO_CLASS_NAME_MAP,
                num_classes=NUM_CLASSES,
                margin_threshold=cfg.inference.aggregation.margin_threshold,
                top_k=cfg.inference.aggregation.top_k_classes,
            )

            # 8. Classification Report
            if final_true_labels:
                print(f"\n{'-'*20} Classification Report {'-'*20}")

                present_labels = sorted(
                    list(set(final_true_labels) | set(final_predicted_labels))
                )
                present_labels = [lbl for lbl in present_labels if lbl >= 0]

                if present_labels:
                    target_names = [
                        LABEL_TO_CLASS_NAME_MAP.get(lbl, f"Label_{lbl}")[:25]
                        for lbl in present_labels
                    ]

                    report_str = classification_report(
                        final_true_labels,
                        final_predicted_labels,
                        labels=present_labels,
                        target_names=target_names,
                        zero_division=0,
                        digits=3,
                    )
                    print(report_str)

                    experiment_name = "LSTM_Inference_Results"
                    try:
                        experiment = mlflow.get_experiment_by_name(experiment_name)
                        if experiment is None:
                            experiment_id = mlflow.create_experiment(experiment_name)
                        else:
                            experiment_id = experiment.experiment_id
                        mlflow.set_experiment(experiment_name)
                    except Exception as e:
                        print(
                            f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç MLflow: {e}"
                        )
                        # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                        experiment_id = None

                    # ‚úÖ –¢–ï–ü–ï–†–¨ –ë–ï–ó–û–ü–ê–°–ù–û –ó–ê–ü–£–°–ö–ê–ï–ú RUN
                    if experiment_id is not None:
                        with mlflow.start_run(
                            run_name="PyTorch_Lightning_LSTM_Inference"
                        ):
                            accuracy = sum(
                                1
                                for t, p in zip(
                                    final_true_labels, final_predicted_labels
                                )
                                if t == p
                            ) / len(final_true_labels)
                            mlflow.log_metric("file_level_accuracy", accuracy)
                            mlflow.log_metric(
                                "total_files_processed", len(file_summaries)
                            )
                            mlflow.log_metric(
                                "total_sequences_processed", len(all_sequence_results)
                            )

                            # –û—Å—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ...
                            print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω—ã –≤ MLflow")
                    else:
                        print(
                            "‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLflow –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"
                        )

                    # 9. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ MLflow
                    with mlflow.start_run(run_name="PyTorch_Lightning_LSTM_Inference"):
                        accuracy = sum(
                            1
                            for t, p in zip(final_true_labels, final_predicted_labels)
                            if t == p
                        ) / len(final_true_labels)
                        mlflow.log_metric("file_level_accuracy", accuracy)
                        mlflow.log_metric("total_files_processed", len(file_summaries))
                        mlflow.log_metric(
                            "total_sequences_processed", len(all_sequence_results)
                        )

                        # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
                        mlflow.log_params(
                            {
                                "margin_threshold": cfg.inference.aggregation.margin_threshold,
                                "batch_size": cfg.inference.batch_size,
                                "aggregation_method": cfg.inference.aggregation.method,
                                "model_type": "PyTorch_Lightning",
                                "endpoint_url": endpoint_url,
                            }
                        )

                        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω—ã –≤ MLflow")

            # 10. –§–ò–ù–ê–õ–¨–ù–´–ô –í–´–í–û–î: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –ª—é–¥–µ–π
            unique_people = set(file_summaries.keys())
            total_people_count = len(unique_people)

            print(f"\nüéØ –ò–¢–û–ì–û–í–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢:")
            print(f"üìä –í—Å–µ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ –ª—é–¥–µ–π: {total_people_count}")
            print(
                f"üìà –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ –ª—é–¥–µ–π: {len([s for s in file_summaries.values() if s['is_correct']])}/{total_people_count}"
            )

            # –ü–æ–∫–∞–∑–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –ª—é–¥–µ–π –ø–æ –∫–ª–∞—Å—Å–∞–º
            predicted_by_class = defaultdict(list)
            for file_id, summary in file_summaries.items():
                predicted_by_class[summary["predicted_name"]].append(file_id)

            print(f"\nüë• –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ –∫–ª–∞—Å—Å–∞–º:")
            for class_name, people in sorted(predicted_by_class.items()):
                print(f"  {class_name}: {len(people)} —á–µ–ª–æ–≤–µ–∫")
        else:
            print("‚ùå –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        traceback.print_exc()
        return 1

    finally:
        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º MLflow serving
        if serving_process:
            print("\nüõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ MLflow serving —Å–µ—Ä–≤–µ—Ä–∞...")
            serving_process.terminate()
            serving_process.wait()
            print("‚úÖ MLflow —Å–µ—Ä–≤–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

    print("\n--- PyTorch Lightning –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –∑–∞–≤–µ—Ä—à–µ–Ω ---")
    return 0


if __name__ == "__main__":
    main()
