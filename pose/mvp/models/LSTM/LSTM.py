"""
LSTM –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ—Ö–æ–¥–∫–∏ –Ω–∞ PyTorch Lightning —Å MLflow –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
–°–æ–≥–ª–∞—Å–Ω–æ Task-2-Training-code.txt
"""

import random
import subprocess
import traceback
from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict, List, Tuple, Union
from warnings import warn

import hydra
import lightning as L
import matplotlib.pyplot as plt
import mlflow
import mlflow.pytorch
import numpy as np
import seaborn as sns
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from hydra import utils
from joblib import dump
from omegaconf import DictConfig, OmegaConf
from sklearn.metrics import (
    ConfusionMatrixDisplay,
    accuracy_score,
    auc,
    classification_report,
    confusion_matrix,
    f1_score,
    precision_score,
    recall_score,
    roc_curve,
)
from sklearn.preprocessing import StandardScaler, label_binarize
from torch.utils.data import DataLoader, Dataset

from ...paths.paths import MODELS, NAMES, TRAIN
from .load import (
    CLASS_NAME_TO_LABEL_MAP,
    CLASS_NAMES_ORDERED,
    NUM_CLASSES,
    create_sequences_from_files,
)

# --- –ò–º–ø–æ—Ä—Ç pandas –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ ---
try:
    import pandas as pd
except ImportError:
    pd = None
    warn(
        "–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ pandas –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ –Ω–µ –±—É–¥–µ—Ç –ø–æ—Å—Ç—Ä–æ–µ–Ω."
    )

if TYPE_CHECKING:
    from pathlib import Path
    from typing import Union


# === –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è ===
def get_git_commit_id() -> str:
    """–ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â–∏–π git commit id —Å–æ–≥–ª–∞—Å–Ω–æ Task-2-Training-code.txt."""
    try:
        commit_id = (
            subprocess.check_output(
                ["git", "rev-parse", "HEAD"], cwd=utils.get_original_cwd()
            )
            .decode("ascii")
            .strip()
        )
        return commit_id
    except Exception as e:
        warn(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å git commit id: {e}")
        return "unknown"


def setup_mlflow(cfg: DictConfig) -> None:
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç MLflow —Å–æ–≥–ª–∞—Å–Ω–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º Task-2-Training-code.txt."""
    if not cfg.training.logging.mlflow.enable:
        return

    try:
        mlflow.set_tracking_uri(cfg.training.logging.mlflow.tracking_uri)
        mlflow.set_experiment(cfg.training.logging.mlflow.experiment_name)
        print(f"‚úÖ MLflow –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {cfg.training.logging.mlflow.tracking_uri}")
        print(f"‚úÖ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {cfg.training.logging.mlflow.experiment_name}")
    except Exception as e:
        warn(f"–û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ MLflow: {e}")


def seed_worker(worker_id: int):
    """–§—É–Ω–∫—Ü–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è –≤–æ—Ä–∫–µ—Ä–æ–≤ DataLoader."""
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)


# === –ö–∞—Å—Ç–æ–º–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç ===
class GaitSequenceDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç PyTorch –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Ö–æ–¥–∫–∏."""

    def __init__(self, sequences: List[torch.Tensor], labels: List[int]):
        if not sequences or not labels or len(sequences) != len(labels):
            raise ValueError(
                "–°–ø–∏—Å–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏ –º–µ—Ç–æ–∫ –Ω–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—É—Å—Ç—ã–º–∏ –∏ –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—É—é –¥–ª–∏–Ω—É."
            )
        self.sequences = sequences
        self.labels = labels
        seq_shape = sequences[0].shape if sequences else "(–ø—É—Å—Ç–æ)"
        print(
            f"–°–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç —Å {len(self.sequences)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏. –ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º—ã: {seq_shape}"
        )

    def __len__(self) -> int:
        return len(self.sequences)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:
        return self.sequences[idx], self.labels[idx]


# === –ú–æ–¥–µ–ª—å LSTM ===
class GaitClassifierLSTM(nn.Module):
    """–ú–æ–¥–µ–ª—å LSTM –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Ö–æ–¥–∫–∏."""

    def __init__(
        self,
        input_size: int,
        hidden_size: int,
        num_layers: int,
        num_classes: int,
        use_bidirectional: bool = True,
        lstm_dropout: float = 0.6,
        use_ffn_head: bool = False,
        ffn_hidden_size: int = 128,
        ffn_dropout: float = 0.6,
    ):
        super(GaitClassifierLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.num_directions = 2 if use_bidirectional else 1
        self.use_ffn_head = use_ffn_head

        self.lstm = nn.LSTM(
            input_size,
            hidden_size,
            num_layers,
            batch_first=True,
            bidirectional=use_bidirectional,
            dropout=lstm_dropout if num_layers > 1 else 0.0,
        )

        classifier_input_size = hidden_size * self.num_directions

        if self.use_ffn_head:
            print(
                f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è FFN –≥–æ–ª–æ–≤–∞ —Å —Ä–∞–∑–º–µ—Ä–æ–º {ffn_hidden_size} –∏ dropout {ffn_dropout}"
            )
            self.classifier_head = nn.Sequential(
                nn.Linear(classifier_input_size, ffn_hidden_size),
                nn.ReLU(),
                nn.Dropout(ffn_dropout),
                nn.Linear(ffn_hidden_size, num_classes),
            )
        else:
            print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–¥–∏–Ω –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.")
            self.dropout_fc = nn.Dropout(ffn_dropout)
            self.classifier_head = nn.Linear(classifier_input_size, num_classes)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        batch_size = x.size(0)

        h0 = torch.zeros(
            self.num_layers * self.num_directions, batch_size, self.hidden_size
        ).to(x.device)
        c0 = torch.zeros(
            self.num_layers * self.num_directions, batch_size, self.hidden_size
        ).to(x.device)

        out, _ = self.lstm(x, (h0, c0))
        last_step_out = out[:, -1, :]

        if self.use_ffn_head:
            logits = self.classifier_head(last_step_out)
        else:
            out_dropout = self.dropout_fc(last_step_out)
            logits = self.classifier_head(out_dropout)

        return logits


# === Focal Loss ===
class FocalLoss(nn.Module):
    """–†–µ–∞–ª–∏–∑–∞—Ü–∏—è Focal Loss –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤."""

    def __init__(
        self,
        alpha: Union[float, list, torch.Tensor] = 1.0,
        gamma: float = 2.0,
        reduction: str = "mean",
    ):
        super(FocalLoss, self).__init__()
        if reduction not in ["mean", "sum", "none"]:
            raise ValueError(
                f"–ù–µ–¥–æ–ø—É—Å—Ç–∏–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ reduction: {reduction}. –î–æ–ø—É—Å—Ç–∏–º—ã 'mean', 'sum', 'none'."
            )
        if gamma < 0:
            raise ValueError("–ü–∞—Ä–∞–º–µ—Ç—Ä gamma –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º.")

        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        num_classes = inputs.size(1)

        log_probs = F.log_softmax(inputs, dim=1)
        probs = torch.exp(log_probs)

        target_one_hot = F.one_hot(targets, num_classes=num_classes).float()
        log_probs_true_class = log_probs.gather(1, targets.unsqueeze(1)).squeeze(1)
        probs_true_class = probs.gather(1, targets.unsqueeze(1)).squeeze(1)

        focal_weights = torch.pow(1 - probs_true_class, self.gamma)

        if isinstance(self.alpha, (list, tuple)):
            if (
                not hasattr(self, "alpha_tensor")
                or self.alpha_tensor.device != inputs.device
            ):
                self.alpha_tensor = torch.tensor(
                    self.alpha, device=inputs.device, dtype=torch.float32
                )
            alpha_weights = self.alpha_tensor.gather(0, targets)
        elif isinstance(self.alpha, torch.Tensor):
            alpha_weights = self.alpha.to(inputs.device).gather(0, targets)
        elif isinstance(self.alpha, (int, float)):
            alpha_weights = self.alpha
        else:
            raise TypeError("–ü–∞—Ä–∞–º–µ—Ç—Ä alpha –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å float, list –∏–ª–∏ torch.Tensor.")

        loss_per_sample = -alpha_weights * focal_weights * log_probs_true_class

        if self.reduction == "mean":
            return loss_per_sample.mean()
        elif self.reduction == "sum":
            return loss_per_sample.sum()
        else:
            return loss_per_sample


# === –§—É–Ω–∫—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤ ===
def save_individual_training_plots(
    lightning_model: "GaitClassifierLightning",
    plot_dir: Path,
    cfg: DictConfig,
    class_names_ordered: List[str] = None,
):
    """–°–æ–∑–¥–∞–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–∏ —Å–æ–≥–ª–∞—Å–Ω–æ Task-2-Training-code.txt."""
    plot_dir.mkdir(exist_ok=True, parents=True)
    print(f"üìä –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –≤ {plot_dir}...")

    try:
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –º–æ–¥–µ–ª–∏
        history = getattr(lightning_model, "training_history", {})
        final_epoch_data = getattr(lightning_model, "final_epoch_data", {})

        plot_paths = []

        # === –ì—Ä–∞—Ñ–∏–∫ 1: Loss (–æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª) ===
        plt.figure(figsize=(10, 6))

        if history.get("train_loss") and history.get("test_loss"):
            # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä—ã –º–∞—Å—Å–∏–≤–æ–≤
            train_loss = history["train_loss"]
            test_loss = history["test_loss"]
            min_len = min(len(train_loss), len(test_loss))

            if min_len > 0:
                epochs = range(1, min_len + 1)
                plt.plot(
                    epochs,
                    train_loss[:min_len],
                    "b-",
                    label="Training Loss",
                    linewidth=2,
                )
                plt.plot(
                    epochs,
                    test_loss[:min_len],
                    "r-",
                    label="Validation Loss",
                    linewidth=2,
                )
                plt.xlabel("–≠–ø–æ—Ö–∞")
                plt.ylabel("–ü–æ—Ç–µ—Ä–∏")
                plt.title("–ò—Å—Ç–æ—Ä–∏—è –ø–æ—Ç–µ—Ä—å –æ–±—É—á–µ–Ω–∏—è")
                plt.legend()
                plt.grid(True, alpha=0.3)
        else:
            plt.text(
                0.5,
                0.5,
                "–ò—Å—Ç–æ—Ä–∏—è –ø–æ—Ç–µ—Ä—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞",
                ha="center",
                va="center",
                fontsize=14,
            )
            plt.title("–ò—Å—Ç–æ—Ä–∏—è –ø–æ—Ç–µ—Ä—å –æ–±—É—á–µ–Ω–∏—è")

        loss_path = plot_dir / "loss_history.png"
        plt.savefig(loss_path, dpi=150, bbox_inches="tight")
        plt.close()
        plot_paths.append(loss_path)
        print(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å: {loss_path}")

        # === –ì—Ä–∞—Ñ–∏–∫ 2: Accuracy (–æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª) ===
        plt.figure(figsize=(10, 6))

        if history.get("train_acc") and history.get("test_acc"):
            # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä—ã –º–∞—Å—Å–∏–≤–æ–≤
            train_acc = history["train_acc"]
            test_acc = history["test_acc"]
            min_len = min(len(train_acc), len(test_acc))

            if min_len > 0:
                epochs = range(1, min_len + 1)
                plt.plot(
                    epochs,
                    train_acc[:min_len],
                    "g-",
                    label="Training Accuracy",
                    linewidth=2,
                )
                plt.plot(
                    epochs,
                    test_acc[:min_len],
                    "m-",
                    label="Validation Accuracy",
                    linewidth=2,
                )
                plt.xlabel("–≠–ø–æ—Ö–∞")
                plt.ylabel("–¢–æ—á–Ω–æ—Å—Ç—å")
                plt.title("–ò—Å—Ç–æ—Ä–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è")
                plt.legend()
                plt.grid(True, alpha=0.3)
                plt.ylim([0, 1])
        else:
            plt.text(
                0.5,
                0.5,
                "–ò—Å—Ç–æ—Ä–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞",
                ha="center",
                va="center",
                fontsize=14,
            )
            plt.title("–ò—Å—Ç–æ—Ä–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è")

        accuracy_path = plot_dir / "accuracy_history.png"
        plt.savefig(accuracy_path, dpi=150, bbox_inches="tight")
        plt.close()
        plot_paths.append(accuracy_path)
        print(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏: {accuracy_path}")

        # === –ì—Ä–∞—Ñ–∏–∫ 3: F1 Score (–æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª) ===
        plt.figure(figsize=(10, 6))

        if history.get("train_f1") and history.get("test_f1"):
            # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä—ã –º–∞—Å—Å–∏–≤–æ–≤
            train_f1 = history["train_f1"]
            test_f1 = history["test_f1"]
            min_len = min(len(train_f1), len(test_f1))

            if min_len > 0:
                epochs = range(1, min_len + 1)
                plt.plot(
                    epochs,
                    train_f1[:min_len],
                    "orange",
                    label="Training F1",
                    linewidth=2,
                )
                plt.plot(
                    epochs,
                    test_f1[:min_len],
                    "purple",
                    label="Validation F1",
                    linewidth=2,
                )
                plt.xlabel("–≠–ø–æ—Ö–∞")
                plt.ylabel("F1 Score")
                plt.title("–ò—Å—Ç–æ—Ä–∏—è F1 Score")
                plt.legend()
                plt.grid(True, alpha=0.3)
                plt.ylim([0, 1])
        else:
            plt.text(
                0.5, 0.5, "–ò—Å—Ç–æ—Ä–∏—è F1 –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞", ha="center", va="center", fontsize=14
            )
            plt.title("–ò—Å—Ç–æ—Ä–∏—è F1 Score")

        f1_path = plot_dir / "f1_history.png"
        plt.savefig(f1_path, dpi=150, bbox_inches="tight")
        plt.close()
        plot_paths.append(f1_path)
        print(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫ F1: {f1_path}")

        # === –ì—Ä–∞—Ñ–∏–∫ 4: Confusion Matrix (–æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª) ===
        plt.figure(figsize=(12, 10))

        if final_epoch_data.get("test_labels") and final_epoch_data.get("test_preds"):
            cm = confusion_matrix(
                final_epoch_data["test_labels"], final_epoch_data["test_preds"]
            )
            display_labels = (
                [name[:12] for name in class_names_ordered]
                if class_names_ordered
                else None
            )
            disp = ConfusionMatrixDisplay(
                confusion_matrix=cm, display_labels=display_labels
            )
            disp.plot(cmap="Blues", xticks_rotation="vertical")
            plt.title("–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (–í–∞–ª–∏–¥–∞—Ü–∏—è)")
        else:
            plt.text(
                0.5,
                0.5,
                "–î–∞–Ω–Ω—ã–µ –¥–ª—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã",
                ha="center",
                va="center",
                fontsize=14,
            )
            plt.title("–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (–í–∞–ª–∏–¥–∞—Ü–∏—è)")

        confusion_path = plot_dir / "confusion_matrix.png"
        plt.savefig(confusion_path, dpi=150, bbox_inches="tight")
        plt.close()
        plot_paths.append(confusion_path)
        print(f"‚úÖ –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫: {confusion_path}")

        # === –ì—Ä–∞—Ñ–∏–∫ 5: Classification Report (–æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª) ===
        fig, ax = plt.subplots(figsize=(12, 8))
        ax.axis("off")

        if final_epoch_data.get("test_labels") and final_epoch_data.get("test_preds"):
            try:
                report = classification_report(
                    final_epoch_data["test_labels"],
                    final_epoch_data["test_preds"],
                    target_names=(
                        [name[:20] for name in class_names_ordered]
                        if class_names_ordered
                        else None
                    ),
                    zero_division=0,
                    digits=3,
                )
                ax.text(
                    0.01,
                    0.99,
                    report,
                    family="monospace",
                    va="top",
                    ha="left",
                    fontsize=10,
                )
                ax.set_title("Classification Report", fontsize=14, pad=20)
            except Exception as e:
                ax.text(
                    0.5,
                    0.5,
                    f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç—á–µ—Ç–∞:\n{e}",
                    ha="center",
                    va="center",
                    color="red",
                )
        else:
            ax.text(
                0.5,
                0.5,
                "–î–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç—á–µ—Ç–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã",
                ha="center",
                va="center",
                fontsize=14,
            )
            ax.set_title("Classification Report", fontsize=14)

        report_path = plot_dir / "classification_report.png"
        plt.savefig(report_path, dpi=150, bbox_inches="tight")
        plt.close()
        plot_paths.append(report_path)
        print(f"‚úÖ Classification Report: {report_path}")

        # === –ì—Ä–∞—Ñ–∏–∫ 6: Model Summary (–æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª) ===
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.axis("off")

        total_params = sum(p.numel() for p in lightning_model.parameters())
        trainable_params = sum(
            p.numel() for p in lightning_model.parameters() if p.requires_grad
        )

        summary_text = f"""
PyTorch Lightning LSTM Model Summary

Architecture:
‚îú‚îÄ Hidden Size: {cfg.training.model.hidden_size}
‚îú‚îÄ Num Layers: {cfg.training.model.num_layers}
‚îú‚îÄ Bidirectional: {cfg.training.model.use_bidirectional}
‚îú‚îÄ LSTM Dropout: {cfg.training.model.lstm_dropout}
‚îú‚îÄ FC Dropout: {cfg.training.model.fc_dropout}

Training Config:
‚îú‚îÄ Epochs: {cfg.training.training.epochs}
‚îú‚îÄ Batch Size: {cfg.training.training.batch_size}
‚îú‚îÄ Learning Rate: {cfg.training.training.learning_rate}
‚îú‚îÄ Optimizer: {cfg.training.training.optimizer.name}

Data Config:
‚îú‚îÄ Sequence Length: {cfg.training.data.sequence_length}
‚îú‚îÄ Input Size per Frame: {cfg.training.data.input_size_per_frame}
‚îú‚îÄ Stride: {cfg.training.data.stride}
‚îú‚îÄ Train Ratio: {cfg.training.data.train_ratio}

Model Parameters:
‚îú‚îÄ Total Parameters: {total_params:,}
‚îú‚îÄ Trainable Parameters: {trainable_params:,}
‚îú‚îÄ Model Size: {total_params * 4 / 1024 / 1024:.2f} MB

Classes: {NUM_CLASSES} gait classes
        """

        ax.text(
            0.05,
            0.95,
            summary_text,
            family="monospace",
            va="top",
            ha="left",
            fontsize=10,
        )
        ax.set_title("Model & Training Summary", fontsize=14, pad=20)

        summary_path = plot_dir / "model_summary.png"
        plt.savefig(summary_path, dpi=150, bbox_inches="tight")
        plt.close()
        plot_paths.append(summary_path)
        print(f"‚úÖ Model Summary: {summary_path}")

        print(f"üìä –í—Å–µ –≥—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {plot_dir}")
        return plot_paths

    except Exception as e:
        warn(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤: {e}")
        traceback.print_exc()
        return []


# === PyTorch Lightning –º–æ–¥—É–ª—å ===
class GaitClassifierLightning(L.LightningModule):
    """PyTorch Lightning –º–æ–¥—É–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ—Ö–æ–¥–∫–∏ —Å LSTM."""

    def __init__(self, cfg: DictConfig, class_names_ordered: List[str] = None):
        super().__init__()
        self.cfg = cfg
        self.class_names_ordered = class_names_ordered or CLASS_NAMES_ORDERED
        self.save_hyperparameters(ignore=["class_names_ordered"])

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        self.model = GaitClassifierLSTM(
            input_size=cfg.training.data.input_size_per_frame,
            hidden_size=cfg.training.model.hidden_size,
            num_layers=cfg.training.model.num_layers,
            num_classes=NUM_CLASSES,
            use_bidirectional=cfg.training.model.use_bidirectional,
            lstm_dropout=cfg.training.model.lstm_dropout,
            use_ffn_head=cfg.training.model.use_ffn_head,
            ffn_hidden_size=cfg.training.model.ffn_hidden_size,
            ffn_dropout=cfg.training.model.fc_dropout,
        )

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∏—Ç–µ—Ä–∏—è –ø–æ—Ç–µ—Ä—å
        if cfg.training.training.loss.name == "focal":
            self.criterion = FocalLoss(
                alpha=cfg.training.training.loss.focal.alpha,
                gamma=cfg.training.training.loss.focal.gamma,
                reduction="mean",
            )
        else:
            self.criterion = nn.CrossEntropyLoss()

        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –°–ë–û–† –ò–°–¢–û–†–ò–ò –ú–ï–¢–†–ò–ö
        self.training_history = {
            "train_loss": [],
            "test_loss": [],
            "train_acc": [],
            "test_acc": [],
            "train_f1": [],
            "test_f1": [],
            "train_precision": [],
            "test_precision": [],
            "train_recall": [],
            "test_recall": [],
        }

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –∑–∞ —ç–ø–æ—Ö—É
        self.epoch_train_metrics = {
            "loss": [],
            "acc": [],
            "f1": [],
            "precision": [],
            "recall": [],
        }
        self.epoch_val_metrics = {
            "loss": [],
            "acc": [],
            "f1": [],
            "precision": [],
            "recall": [],
        }

        # –î–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–Ω–µ–π —ç–ø–æ—Ö–∏
        self.final_epoch_data = {
            "train_labels": [],
            "train_preds": [],
            "train_probs": [],
            "test_labels": [],
            "test_preds": [],
            "test_probs": [],
        }

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        sequences, labels = batch
        outputs = self(sequences)
        loss = self.criterion(outputs, labels)

        # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
        with torch.no_grad():
            probs = torch.softmax(outputs, dim=1)
            _, preds = torch.max(outputs, 1)
            acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())
            f1 = f1_score(
                labels.cpu().numpy(),
                preds.cpu().numpy(),
                average="weighted",
                zero_division=0,
            )
            precision = precision_score(
                labels.cpu().numpy(),
                preds.cpu().numpy(),
                average="weighted",
                zero_division=0,
            )
            recall = recall_score(
                labels.cpu().numpy(),
                preds.cpu().numpy(),
                average="weighted",
                zero_division=0,
            )

        # ‚úÖ –ù–ê–ö–ê–ü–õ–ò–í–ê–ï–ú –ú–ï–¢–†–ò–ö–ò –ó–ê –≠–ü–û–•–£
        self.epoch_train_metrics["loss"].append(loss.item())
        self.epoch_train_metrics["acc"].append(acc)
        self.epoch_train_metrics["f1"].append(f1)
        self.epoch_train_metrics["precision"].append(precision)
        self.epoch_train_metrics["recall"].append(recall)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —Å–æ–≥–ª–∞—Å–Ω–æ Task-2-Training-code.txt
        self.log("train_loss", loss, on_step=False, on_epoch=True, prog_bar=True)
        self.log("train_accuracy", acc, on_step=False, on_epoch=True, prog_bar=True)
        self.log("train_f1", f1, on_step=False, on_epoch=True)
        self.log("train_precision", precision, on_step=False, on_epoch=True)
        self.log("train_recall", recall, on_step=False, on_epoch=True)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–π —ç–ø–æ—Ö–∏
        if self.current_epoch == self.cfg.training.training.epochs - 1:
            self.final_epoch_data["train_labels"].extend(labels.cpu().numpy())
            self.final_epoch_data["train_preds"].extend(preds.cpu().numpy())
            self.final_epoch_data["train_probs"].extend(probs.cpu().numpy())

        return loss

    def validation_step(self, batch, batch_idx):
        sequences, labels = batch
        outputs = self(sequences)
        loss = self.criterion(outputs, labels)

        # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
        with torch.no_grad():
            probs = torch.softmax(outputs, dim=1)
            _, preds = torch.max(outputs, 1)
            acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())
            f1 = f1_score(
                labels.cpu().numpy(),
                preds.cpu().numpy(),
                average="weighted",
                zero_division=0,
            )
            precision = precision_score(
                labels.cpu().numpy(),
                preds.cpu().numpy(),
                average="weighted",
                zero_division=0,
            )
            recall = recall_score(
                labels.cpu().numpy(),
                preds.cpu().numpy(),
                average="weighted",
                zero_division=0,
            )

        # ‚úÖ –ù–ê–ö–ê–ü–õ–ò–í–ê–ï–ú –ú–ï–¢–†–ò–ö–ò –ó–ê –≠–ü–û–•–£
        self.epoch_val_metrics["loss"].append(loss.item())
        self.epoch_val_metrics["acc"].append(acc)
        self.epoch_val_metrics["f1"].append(f1)
        self.epoch_val_metrics["precision"].append(precision)
        self.epoch_val_metrics["recall"].append(recall)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        self.log("val_loss", loss, on_step=False, on_epoch=True, prog_bar=True)
        self.log("val_accuracy", acc, on_step=False, on_epoch=True, prog_bar=True)
        self.log("val_f1", f1, on_step=False, on_epoch=True)
        self.log("val_precision", precision, on_step=False, on_epoch=True)
        self.log("val_recall", recall, on_step=False, on_epoch=True)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–π —ç–ø–æ—Ö–∏
        if self.current_epoch == self.cfg.training.training.epochs - 1:
            self.final_epoch_data["test_labels"].extend(labels.cpu().numpy())
            self.final_epoch_data["test_preds"].extend(preds.cpu().numpy())
            self.final_epoch_data["test_probs"].extend(probs.cpu().numpy())

        return loss

    def configure_optimizers(self):
        if self.cfg.training.training.optimizer.name.lower() == "adamw":
            optimizer = optim.AdamW(
                self.parameters(),
                lr=self.cfg.training.training.learning_rate,
                weight_decay=self.cfg.training.training.weight_decay,
            )
        else:
            optimizer = optim.Adam(
                self.parameters(), lr=self.cfg.training.training.learning_rate
            )
        return optimizer

    # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –°–ë–û–† –ú–ï–¢–†–ò–ö –ü–û –≠–ü–û–•–ê–ú (–ü–†–û–ü–£–°–ö–ê–ï–ú SANITY CHECK)
    def on_train_epoch_end(self):
        """–°–æ–±–∏—Ä–∞–µ–º —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è –∑–∞ —ç–ø–æ—Ö—É"""
        # ‚úÖ –ü–†–û–ü–£–°–ö–ê–ï–ú SANITY CHECK
        if self.trainer.sanity_checking:
            return

        if self.epoch_train_metrics["loss"]:
            # –£—Å—Ä–µ–¥–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∑–∞ —ç–ø–æ—Ö—É
            avg_train_loss = np.mean(self.epoch_train_metrics["loss"])
            avg_train_acc = np.mean(self.epoch_train_metrics["acc"])
            avg_train_f1 = np.mean(self.epoch_train_metrics["f1"])
            avg_train_precision = np.mean(self.epoch_train_metrics["precision"])
            avg_train_recall = np.mean(self.epoch_train_metrics["recall"])

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
            self.training_history["train_loss"].append(avg_train_loss)
            self.training_history["train_acc"].append(avg_train_acc)
            self.training_history["train_f1"].append(avg_train_f1)
            self.training_history["train_precision"].append(avg_train_precision)
            self.training_history["train_recall"].append(avg_train_recall)

            # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            self.epoch_train_metrics = {
                "loss": [],
                "acc": [],
                "f1": [],
                "precision": [],
                "recall": [],
            }

            print(
                f"–≠–ø–æ—Ö–∞ {self.current_epoch}: Train Loss={avg_train_loss:.4f}, Train Acc={avg_train_acc:.4f}"
            )

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLflow —á–µ—Ä–µ–∑ Lightning
        if self.cfg.training.logging.mlflow.enable:
            try:
                current_lr = self.optimizers().param_groups[0]["lr"]
                self.log("learning_rate", current_lr)
            except Exception as e:
                warn(f"–û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ MLflow: {e}")

    def on_validation_epoch_end(self):
        """–°–æ–±–∏—Ä–∞–µ–º —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∑–∞ —ç–ø–æ—Ö—É"""
        # ‚úÖ –ü–†–û–ü–£–°–ö–ê–ï–ú SANITY CHECK
        if self.trainer.sanity_checking:
            return

        if self.epoch_val_metrics["loss"]:
            # –£—Å—Ä–µ–¥–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∑–∞ —ç–ø–æ—Ö—É
            avg_val_loss = np.mean(self.epoch_val_metrics["loss"])
            avg_val_acc = np.mean(self.epoch_val_metrics["acc"])
            avg_val_f1 = np.mean(self.epoch_val_metrics["f1"])
            avg_val_precision = np.mean(self.epoch_val_metrics["precision"])
            avg_val_recall = np.mean(self.epoch_val_metrics["recall"])

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
            self.training_history["test_loss"].append(avg_val_loss)
            self.training_history["test_acc"].append(avg_val_acc)
            self.training_history["test_f1"].append(avg_val_f1)
            self.training_history["test_precision"].append(avg_val_precision)
            self.training_history["test_recall"].append(avg_val_recall)

            # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            self.epoch_val_metrics = {
                "loss": [],
                "acc": [],
                "f1": [],
                "precision": [],
                "recall": [],
            }

            print(
                f"–≠–ø–æ—Ö–∞ {self.current_epoch}: Val Loss={avg_val_loss:.4f}, Val Acc={avg_val_acc:.4f}"
            )

    def on_fit_end(self):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ –æ–±—É—á–µ–Ω–∏—è - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫–∏ —Å–æ–≥–ª–∞—Å–Ω–æ Task-2-Training-code.txt"""
        if self.cfg.training.saving.save_plots:
            try:
                original_cwd = Path(utils.get_original_cwd())
                plot_dir = (
                    original_cwd
                    / self.cfg.data.paths.plots_dir
                    / self.cfg.training.saving.plots_dirname
                )

                print(f"\nüìä –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –≤ {plot_dir}...")

                plot_paths = save_individual_training_plots(
                    self, plot_dir, self.cfg, self.class_names_ordered
                )

                # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ì–†–ê–§–ò–ö–û–í –í MLFLOW
                if self.cfg.training.logging.mlflow.enable and hasattr(
                    self.logger, "experiment"
                ):
                    try:
                        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π run_id –∏–∑ Lightning logger
                        current_run = self.logger.experiment.get_run(self.logger.run_id)

                        # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –≥—Ä–∞—Ñ–∏–∫ –æ—Ç–¥–µ–ª—å–Ω–æ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º API
                        for plot_path in plot_paths:
                            if plot_path.exists():
                                # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–´–ô –í–´–ó–û–í - –∏—Å–ø–æ–ª—å–∑—É–µ–º run_id –∏–∑ logger
                                self.logger.experiment.log_artifact(
                                    run_id=self.logger.run_id,
                                    local_path=str(plot_path),
                                    artifact_path="plots",
                                )
                                print(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫ –∑–∞–≥—Ä—É–∂–µ–Ω –≤ MLflow: {plot_path.name}")
                    except Exception as e:
                        warn(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –≤ MLflow: {e}")
                        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± —á–µ—Ä–µ–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π mlflow
                        try:
                            for plot_path in plot_paths:
                                if plot_path.exists():
                                    mlflow.log_artifact(str(plot_path), "plots")
                                    print(
                                        f"‚úÖ –ì—Ä–∞—Ñ–∏–∫ –∑–∞–≥—Ä—É–∂–µ–Ω –≤ MLflow (fallback): {plot_path.name}"
                                    )
                        except Exception as fallback_error:
                            warn(f"Fallback —Ç–∞–∫–∂–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {fallback_error}")

            except Exception as e:
                warn(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤: {e}")


# –û–±–Ω–æ–≤–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é export_model_to_onnx –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:


def export_model_to_onnx(
    lightning_model: GaitClassifierLightning, cfg: DictConfig, original_cwd: Path
) -> Path:
    """
    –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é Lightning –º–æ–¥–µ–ª—å –≤ ONNX —Ñ–æ—Ä–º–∞—Ç —Å–æ–≥–ª–∞—Å–Ω–æ Task-2-Training-code.txt.
    """
    print("üîÑ –≠–∫—Å–ø–æ—Ä—Ç LSTM –º–æ–¥–µ–ª–∏ –≤ ONNX...")

    # –ü—É—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    models_dir = original_cwd / cfg.data.paths.models_dir
    onnx_dir = models_dir / "LSTM" / "ONNX"
    onnx_dir.mkdir(parents=True, exist_ok=True)

    onnx_path = onnx_dir / "lstm_gait_classifier.onnx"

    # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏
    lightning_model.eval()

    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    example_input = torch.randn(
        1,  # batch_size = 1 –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
        cfg.training.data.sequence_length,  # 30
        cfg.training.data.input_size_per_frame,  # 84
    )

    print(f"üìù –§–æ—Ä–º–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {example_input.shape}")
    print(f"üìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤: {onnx_path}")

    # ‚úÖ –ò–°–ü–û–õ–¨–ó–£–ï–ú –ü–ê–†–ê–ú–ï–¢–†–´ –ò–ó –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò
    opset_version = (
        getattr(cfg.training, "production", {}).get("onnx", {}).get("opset_version", 11)
    )
    optimize = (
        getattr(cfg.training, "production", {}).get("onnx", {}).get("optimize", True)
    )

    # –≠–∫—Å–ø–æ—Ä—Ç –≤ ONNX
    with torch.no_grad():
        torch.onnx.export(
            lightning_model.model,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é LSTM –º–æ–¥–µ–ª—å
            example_input,
            str(onnx_path),
            export_params=True,
            opset_version=opset_version,  # –ò–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            do_constant_folding=optimize,  # –ò–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            input_names=["input_sequences"],
            output_names=["class_predictions"],
            dynamic_axes={
                "input_sequences": {0: "batch_size"},  # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π batch size
                "class_predictions": {0: "batch_size"},
            },
            verbose=False,
        )

    print(f"‚úÖ ONNX –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {onnx_path}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–∑–¥–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    try:
        import onnx

        onnx_model = onnx.load(str(onnx_path))
        onnx.checker.check_model(onnx_model)
        print("‚úÖ ONNX –º–æ–¥–µ–ª—å –ø—Ä–æ—à–ª–∞ –ø—Ä–æ–≤–µ—Ä–∫—É")

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
        file_size = onnx_path.stat().st_size / 1024 / 1024
        print(f"üìä –†–∞–∑–º–µ—Ä ONNX —Ñ–∞–π–ª–∞: {file_size:.2f} MB")
        print(f"üìä ONNX opset version: {opset_version}")

    except ImportError:
        warn("ONNX –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–æ–¥–µ–ª–∏")
    except Exception as e:
        warn(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ ONNX –º–æ–¥–µ–ª–∏: {e}")

    return onnx_path


# === –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ===
@hydra.main(config_path="../../../../configs", config_name="config", version_base="1.1")
def main(cfg: DictConfig) -> None:
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å PyTorch Lightning –∏ MLflow –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π —Å–æ–≥–ª–∞—Å–Ω–æ Task-2-Training-code.txt."""
    if torch.cuda.is_available():
        torch.set_float32_matmul_precision("medium")
        device_name = torch.cuda.get_device_name(torch.cuda.current_device())
        print(f"‚úÖ GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–∫–ª—é—á–µ–Ω–∞ –¥–ª—è {device_name} (medium precision)")

    print("=== LSTM –æ–±—É—á–µ–Ω–∏–µ —Å PyTorch Lightning + MLflow ===")
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {NUM_CLASSES} –∫–ª–∞—Å—Å–æ–≤: {CLASS_NAMES_ORDERED}")
    print(OmegaConf.to_yaml(cfg.training))
    print("=" * 50)

    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Ç–µ–π —Å–æ–≥–ª–∞—Å–Ω–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º
    original_cwd = Path(utils.get_original_cwd())
    models_dir = original_cwd / cfg.data.paths.models_dir
    weights_dir = models_dir / "LSTM" / "LSTM_weights"
    weights_dir.mkdir(parents=True, exist_ok=True)

    scaler_path = weights_dir / cfg.training.saving.scaler_filename
    best_weights_path = weights_dir / cfg.training.saving.model_filename

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ MLflow (—Ç–æ–ª—å–∫–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ URI, –±–µ–∑ —Å–æ–∑–¥–∞–Ω–∏—è run)
    setup_mlflow(cfg)

    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    L.seed_everything(cfg.training.reproducibility.random_seed, workers=True)
    print(f"[ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω Seed : {cfg.training.reproducibility.random_seed} ]")

    # === –ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ===
    print("\n--- –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ---")

    try:
        sequences, labels = create_sequences_from_files(
            data_path=TRAIN,
            sequence_length=cfg.training.data.sequence_length,
            stride=cfg.training.data.stride,
            names=NAMES,
            class_name_to_label_map=CLASS_NAME_TO_LABEL_MAP,
        )
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(sequences)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
    except Exception as e:
        print(f"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return

    # === –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –î–ê–ù–ù–´–• ===
    print("\n--- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö ---")

    all_sequences_np = torch.cat(sequences, dim=0).numpy()
    scaler = StandardScaler()
    scaler.fit(all_sequences_np)

    normalized_sequences = []
    for seq in sequences:
        seq_np = seq.numpy()
        seq_normalized = scaler.transform(seq_np)
        normalized_sequences.append(torch.tensor(seq_normalized, dtype=torch.float32))

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ scaler
    if cfg.training.saving.save_scaler:
        dump(scaler, scaler_path)
        print(f"StandardScaler —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {scaler_path}")

    # === –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–• ===
    print("\n--- –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö ---")

    total_sequences = len(normalized_sequences)
    train_size = int(cfg.training.data.train_ratio * total_sequences)

    indices = list(range(total_sequences))
    random.shuffle(indices)

    train_indices = indices[:train_size]
    test_indices = indices[train_size:]

    train_sequences = [normalized_sequences[i] for i in train_indices]
    train_labels = [labels[i] for i in train_indices]
    test_sequences = [normalized_sequences[i] for i in test_indices]
    test_labels = [labels[i] for i in test_indices]

    print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(train_sequences)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
    print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(test_sequences)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")

    # === –°–û–ó–î–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–û–í –ò DATALOADER'–û–í ===
    print("\n--- –°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤ ---")

    train_dataset = GaitSequenceDataset(train_sequences, train_labels)
    test_dataset = GaitSequenceDataset(test_sequences, test_labels)

    train_loader = DataLoader(
        train_dataset,
        batch_size=cfg.training.training.batch_size,
        shuffle=cfg.training.dataloader.shuffle_train,
        num_workers=cfg.training.dataloader.num_workers,
        pin_memory=cfg.training.dataloader.pin_memory,
        worker_init_fn=seed_worker,
        persistent_workers=True,
    )

    val_loader = DataLoader(
        test_dataset,
        batch_size=cfg.training.training.batch_size,
        shuffle=cfg.training.dataloader.shuffle_test,
        num_workers=cfg.training.dataloader.num_workers,
        pin_memory=cfg.training.dataloader.pin_memory,
        worker_init_fn=seed_worker,
        persistent_workers=True,
    )

    # === –°–û–ó–î–ê–ù–ò–ï LIGHTNING –ú–û–î–ï–õ–ò ===
    print("\n--- –°–æ–∑–¥–∞–Ω–∏–µ PyTorch Lightning –º–æ–¥–µ–ª–∏ ---")

    lightning_model = GaitClassifierLightning(
        cfg=cfg, class_names_ordered=CLASS_NAMES_ORDERED
    )

    total_params = sum(p.numel() for p in lightning_model.parameters())
    trainable_params = sum(
        p.numel() for p in lightning_model.parameters() if p.requires_grad
    )

    print(f"Lightning –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")
    print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
    print(f"–û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {trainable_params:,}")

    # === –ù–ê–°–¢–†–û–ô–ö–ê TRAINER ===
    print("\n--- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Lightning Trainer ---")

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ callbacks
    callbacks = []

    # ModelCheckpoint –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    if cfg.training.saving.save_weights:
        checkpoint_callback = L.pytorch.callbacks.ModelCheckpoint(
            dirpath=weights_dir,
            filename=cfg.training.saving.model_filename.replace(".pth", ""),
            monitor="val_loss",
            save_top_k=1,
            mode="min",
            save_weights_only=True,
        )
        callbacks.append(checkpoint_callback)

    # ‚úÖ –ò–°–ü–û–õ–¨–ó–£–ï–ú –¢–û–õ–¨–ö–û Lightning MLFlowLogger
    logger = None
    if cfg.training.logging.mlflow.enable:
        try:
            from lightning.pytorch.loggers import MLFlowLogger

            logger = MLFlowLogger(
                experiment_name=cfg.training.logging.mlflow.experiment_name,
                tracking_uri=cfg.training.logging.mlflow.tracking_uri,
                run_name=f"LSTM_Training_{cfg.training.model.hidden_size}h_{cfg.training.training.epochs}e",
                # ‚úÖ –î–û–ë–ê–í–õ–Ø–ï–ú –¢–ï–ì–ò –ò –ü–ê–†–ê–ú–ï–¢–†–´ –ß–ï–†–ï–ó LOGGER
                tags={
                    "model_type": "LSTM",
                    "task": "gait_classification",
                    "framework": "pytorch_lightning",
                    "stage": "training",
                },
            )

            # ‚úÖ –õ–û–ì–ò–†–£–ï–ú –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–´ –ß–ï–†–ï–ó LOGGER
            logger.log_hyperparams(
                {
                    "model_hidden_size": cfg.training.model.hidden_size,
                    "model_num_layers": cfg.training.model.num_layers,
                    "model_bidirectional": cfg.training.model.use_bidirectional,
                    "lstm_dropout": cfg.training.model.lstm_dropout,
                    "fc_dropout": cfg.training.model.fc_dropout,
                    "batch_size": cfg.training.training.batch_size,
                    "learning_rate": cfg.training.training.learning_rate,
                    "epochs": cfg.training.training.epochs,
                    "sequence_length": cfg.training.data.sequence_length,
                    "train_ratio": cfg.training.data.train_ratio,
                    "optimizer": cfg.training.training.optimizer.name,
                    "loss_function": cfg.training.training.loss.name,
                    "git_commit_id": get_git_commit_id(),
                }
            )

        except ImportError:
            warn("MLFlowLogger –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ")

    # –°–æ–∑–¥–∞–Ω–∏–µ Trainer
    trainer = L.Trainer(
        max_epochs=cfg.training.training.epochs,
        accelerator="auto",
        devices="auto",
        logger=logger,  # ‚úÖ –ü–ï–†–ï–î–ê–ï–ú LIGHTNING LOGGER
        callbacks=callbacks,
        deterministic=cfg.training.reproducibility.deterministic,
        enable_progress_bar=cfg.training.logging.verbose,
        log_every_n_steps=50,
    )

    # === –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò ===
    print("\n--- –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —Å PyTorch Lightning ---")

    try:
        trainer.fit(
            model=lightning_model,
            train_dataloaders=train_loader,
            val_dataloaders=val_loader,
        )

        print("‚úÖ –û–±—É—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

        # ‚úÖ –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ê–†–¢–ï–§–ê–ö–¢–û–í –ß–ï–†–ï–ó LIGHTNING LOGGER
        if cfg.training.logging.mlflow.enable and logger:
            try:
                from mlflow.models import infer_signature

                # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                example_input = torch.randn(
                    1,  # batch_size
                    cfg.training.data.sequence_length,  # 30
                    cfg.training.data.input_size_per_frame,  # 84
                )

                # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–∏–º–µ—Ä –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                lightning_model.eval()
                with torch.no_grad():
                    example_output = lightning_model(example_input)

                signature = infer_signature(
                    example_input.numpy(), example_output.numpy()
                )

                mlflow.pytorch.log_model(
                    lightning_model,
                    "model",
                    registered_model_name="LSTM_Gait_Classifier_Lightning",
                    signature=signature,
                    input_example=example_input.numpy(),
                )

                print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å signature")

            except Exception as e:
                warn(f"–û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤ MLflow: {e}")

    except Exception as e:
        print(f"\n–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏:")
        traceback.print_exc()
        return

    # === –≠–ö–°–ü–û–†–¢ –í PRODUCTION –§–û–†–ú–ê–¢–´ ===
    production_config = getattr(cfg.training, "production", {})
    onnx_config = production_config.get("onnx", {})

    if cfg.training.saving.save_weights and onnx_config.get("enable", True):
        try:
            # –≠–∫—Å–ø–æ—Ä—Ç –≤ ONNX
            onnx_path = export_model_to_onnx(lightning_model, cfg, original_cwd)

            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ONNX –≤ MLflow
            if cfg.training.logging.mlflow.enable:
                mlflow.log_artifact(str(onnx_path), "models")
                print(f"‚úÖ ONNX –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ MLflow: {onnx_path}")

            print("‚úÖ Production —ç–∫—Å–ø–æ—Ä—Ç –≤ ONNX –∑–∞–≤–µ—Ä—à–µ–Ω")

        except Exception as e:
            warn(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ ONNX: {e}")
            traceback.print_exc()

    print("\n--- –°–∫—Ä–∏–ø—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω —Å ONNX —ç–∫—Å–ø–æ—Ä—Ç–æ–º ---")
    print("\n--- –°–∫—Ä–∏–ø—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω —Å PyTorch Lightning + MLflow ---")


if __name__ == "__main__":
    main()
